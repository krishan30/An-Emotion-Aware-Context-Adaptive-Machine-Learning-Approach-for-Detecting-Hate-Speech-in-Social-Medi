{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "H8LlNprqVf-e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQYVzwmPPO5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aeb955a-862d-432b-d76d-070617933e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install accelerate -U\n",
        "!pip install torchmetrics\n",
        "!pip install optuna\n",
        "!pip install -U \"neptune[optuna]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel,AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from torchmetrics.classification import BinaryAccuracy,BinaryConfusionMatrix,BinaryF1Score,BinaryPrecision,BinaryRecall, MulticlassPrecision,MulticlassRecall,MulticlassF1Score\n",
        "from torchmetrics.collections import MetricCollection\n",
        "from google.colab import userdata\n",
        "import neptune\n",
        "import neptune.integrations.optuna as npt_utils\n",
        "from neptune.types import File\n",
        "import time"
      ],
      "metadata": {
        "id": "DhBv7znKPWf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the GPU or CPU"
      ],
      "metadata": {
        "id": "LfdjDQRGiiwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"NEPTUNE_API_TOKEN\"] = userdata.get('NEPTUNE_API_TOKEN')\n",
        "device = torch.device(\"cuda:0\"  if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pNLafgjrijy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Variable Values"
      ],
      "metadata": {
        "id": "X6Bss8vmWJOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Transformer Model Name\n",
        "bert_model_name = \"bert-base-cased\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "3b3VUKVJizHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Hugging Face Dataset Name\n",
        "dataset_name = \"krishan-CSE/Davidson_Hate_Speech\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "o6t8YE-HWReL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Transformer Model Tokenizer Max Padding Length\n",
        "SEED = 1234\n",
        "PADDING_MAX_LENGTH = 45 # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "bkBs6keDioqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configuring Neptuna"
      ],
      "metadata": {
        "id": "vALRUeEfVkZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Neptuna Project Name,Study ID and Best Trial ID\n",
        "study_id = \"FIN-393\" # @param {type:\"string\"}\n",
        "trial_id = \"FIN-490\" # @param {type:\"string\"}\n",
        "project_name='krishanchavinda.official/Fine-Tuning-DCL-Framework'# @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "UCDv1vyx4mkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the Study Run"
      ],
      "metadata": {
        "id": "n0aemVMh2UId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_study = neptune.init_run(with_id=study_id,project=project_name)"
      ],
      "metadata": {
        "id": "KnFs880H2XWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Best Trial Run"
      ],
      "metadata": {
        "id": "cKxSg6JF2YFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_trial = neptune.init_run(with_id=trial_id,project=project_name)"
      ],
      "metadata": {
        "id": "gy4i2Mjr2b39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_prams=run_trial[\"parameters\"].fetch()"
      ],
      "metadata": {
        "id": "c6J4ZB_y75ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Random Seed for Reproducibility"
      ],
      "metadata": {
        "id": "Lgdhzculitfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed:int):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n"
      ],
      "metadata": {
        "id": "2ppI0D_stchR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_seed(SEED)"
      ],
      "metadata": {
        "id": "TXflz7C-iuDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Test Dataset"
      ],
      "metadata": {
        "id": "JN5cAM4gi2eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "AlVYc1VdtYth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "bxxVYTVltnBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Tokernizer for the Transformer Model"
      ],
      "metadata": {
        "id": "1djnIXHai7J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)"
      ],
      "metadata": {
        "id": "kTEhbiFoi7wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define the Tokenizer Function"
      ],
      "metadata": {
        "id": "byzLJSHYXd7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer.batch_encode_plus(examples[\"text\"], padding='max_length',max_length=PADDING_MAX_LENGTH,add_special_tokens=True,truncation=True)"
      ],
      "metadata": {
        "id": "yio7aiUai_Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the Dataset"
      ],
      "metadata": {
        "id": "HfUlH-6SjCdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "WN0mJKzQjDau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Unwanted Coloumns"
      ],
      "metadata": {
        "id": "H80ECEiRjCXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets=tokenized_datasets.remove_columns(['text'])"
      ],
      "metadata": {
        "id": "OpUvx3hfjKRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format the coloumns"
      ],
      "metadata": {
        "id": "yUrvQSIfjNYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets=tokenized_datasets.with_format(\"torch\")"
      ],
      "metadata": {
        "id": "6_EJBG1YjOEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "fXJwia8Efwqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataLoaders for Train & Test Datasets"
      ],
      "metadata": {
        "id": "FSlxzFEyjRVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader=DataLoader(tokenized_datasets[\"train\"], batch_size=best_prams[\"BATCH_SIZE\"] , shuffle=False)"
      ],
      "metadata": {
        "id": "9qycWMxuGWgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader=DataLoader(tokenized_datasets[\"test\"], batch_size=best_prams[\"BATCH_SIZE\"] , shuffle=False)"
      ],
      "metadata": {
        "id": "V47AijTUjVUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Dual Contrastive Learning Architecture"
      ],
      "metadata": {
        "id": "XfWJrLM0jbqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCLArchitecture(nn.Module):\n",
        "    def __init__(self,dropout:float,bert_model_name:str='bert-base-cased'):\n",
        "        super(DCLArchitecture, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
        "        self.dim = 768\n",
        "        self.dense = nn.Linear(self.dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,batch_tokenized, if_train=False):\n",
        "        input_ids = batch_tokenized['input_ids']\n",
        "        attention_mask = batch_tokenized['attention_mask']\n",
        "        bert_output = self.bert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        bert_cls_hidden_state = bert_output[1]\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if if_train:\n",
        "            bert_cls_hidden_state_aug = self.dropout(bert_cls_hidden_state)\n",
        "            bert_cls_hidden_state = torch.cat((bert_cls_hidden_state, bert_cls_hidden_state_aug), dim=1).reshape(-1, self.dim)\n",
        "        else:\n",
        "            bert_cls_hidden_state = self.dropout(bert_cls_hidden_state)\n",
        "\n",
        "        linear_output = self.dense(bert_cls_hidden_state)\n",
        "        linear_output = linear_output.squeeze(1)\n",
        "\n",
        "        return bert_cls_hidden_state, linear_output"
      ],
      "metadata": {
        "id": "ki5wFpdmjczU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Focal Loss"
      ],
      "metadata": {
        "id": "75utt_iVjhFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha:float=0.4, gamma:float=2, size_average:bool=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = torch.tensor(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "\n",
        "        device = target.device\n",
        "        self.alpha = self.alpha.to(device)\n",
        "\n",
        "        pred = nn.Sigmoid()(pred)\n",
        "        pred = pred.view(-1, 1)\n",
        "        target = target.view(-1, 1)\n",
        "        pred = torch.cat((1-pred, pred), dim=1)\n",
        "\n",
        "        class_mask = torch.zeros(pred.shape[0], pred.shape[1]).to(device)\n",
        "        class_mask.scatter_(1, target.view(-1, 1).long(), 1.)\n",
        "        probs = (pred * class_mask).sum(dim=1).view(-1, 1)\n",
        "        probs = probs.clamp(min=0.0001, max=1.0)\n",
        "\n",
        "        log_p = probs.log()\n",
        "        alpha = torch.ones(pred.shape[0], pred.shape[1]).to(device)\n",
        "        alpha[:, 0] = alpha[:, 0] * (1 - self.alpha)\n",
        "        alpha[:, 1] = alpha[:, 1] * self.alpha\n",
        "        alpha = (alpha * class_mask).sum(dim=1).view(-1, 1)\n",
        "\n",
        "        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p\n",
        "\n",
        "        if self.size_average:\n",
        "            loss = batch_loss.mean()\n",
        "        else:\n",
        "            loss = batch_loss.sum()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "STWD75vgjgcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Unsupervised Contrastive loss"
      ],
      "metadata": {
        "id": "Yu68WCnGHQhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simcse_loss(batch_emb,temp_1:float):\n",
        "    batch_size = batch_emb.size(0)\n",
        "    y_true = torch.cat([torch.arange(1, batch_size, step=2, dtype=torch.long).unsqueeze(1),\n",
        "                        torch.arange(0, batch_size, step=2, dtype=torch.long).unsqueeze(1)],\n",
        "                       dim=1).reshape([batch_size,]).to(device)\n",
        "    norm_emb = F.normalize(batch_emb, dim=1, p=2)\n",
        "    sim_score = torch.matmul(norm_emb, norm_emb.transpose(0,1))\n",
        "    sim_score = sim_score - (torch.eye(batch_size) * 1e12).to(device)\n",
        "    sim_score = sim_score / temp_1\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    loss = loss_func(sim_score, y_true)\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "w64zOjZWHRHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Supervised Contrastive loss"
      ],
      "metadata": {
        "id": "hnnu_eF9HSlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sup_simcse_loss(batch_emb, label,temp_2:float):\n",
        "    n = batch_emb.size(0)\n",
        "\n",
        "    similarity_matrix = F.cosine_similarity(batch_emb.unsqueeze(1), batch_emb.unsqueeze(0), dim=2)\n",
        "    mask = torch.ones_like(similarity_matrix) * (label.expand(n, n).eq(label.expand(n, n).t()))\n",
        "\n",
        "    mask_no_sim = torch.ones_like(mask) - mask\n",
        "    mask_dui_jiao_0 = ((torch.ones(n,n) - torch.eye(n,n)) * 1e12).to(device)\n",
        "    similarity_matrix = torch.exp(similarity_matrix/temp_2)\n",
        "    similarity_matrix = similarity_matrix * mask_dui_jiao_0\n",
        "\n",
        "    sim = mask*similarity_matrix\n",
        "    no_sim = similarity_matrix - sim\n",
        "    no_sim_sum = torch.sum(no_sim , dim=1)\n",
        "    no_sim_sum_expend = no_sim_sum.repeat(n, 1).T\n",
        "\n",
        "    sim_sum  = sim + no_sim_sum_expend\n",
        "    loss = torch.div(sim , sim_sum)\n",
        "    loss = mask_no_sim + loss + (torch.eye(n, n)/1e12).to(device)\n",
        "    loss = -torch.log(loss)\n",
        "    loss = torch.sum(torch.sum(loss, dim=1))/(2*n)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "T4havNp6HUaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configuring the Model & Focal Loss"
      ],
      "metadata": {
        "id": "fC2OGx9MHeN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DCLArchitecture(bert_model_name=bert_model_name,dropout=best_prams[\"DROPOUT\"])\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "sAyWidn5Hi6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(),lr = best_prams[\"lr\"])"
      ],
      "metadata": {
        "id": "9Ll2G9CJ4u5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criteon = FocalLoss(best_prams[\"ALPHA\"],best_prams[\"GAMMA\"])"
      ],
      "metadata": {
        "id": "wBxmL_mPHnUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the Model"
      ],
      "metadata": {
        "id": "2frED-w_HX3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format the Labels"
      ],
      "metadata": {
        "id": "edHXzdE-Gt8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_label(labels):\n",
        "    labels = labels.unsqueeze(1)\n",
        "    labels = torch.cat((labels, labels), dim=1).reshape(-1, 1).squeeze(1)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "zD2j-EUHGzFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training Loop"
      ],
      "metadata": {
        "id": "wcWhQdRiG2AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, train_dataloader,optimizer,criteon,device,temp_1:float,temp_2:float,lamda:float):\n",
        "    accuracy_metric = BinaryAccuracy()\n",
        "    accuracy_metric.to(device)\n",
        "    progress_bar = tqdm(range(len(train_dataloader)))\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        labels = copy_label(batch[\"labels\"])\n",
        "        emb, pred = model(batch, if_train=True)\n",
        "        loss = criteon(pred, labels.float())\n",
        "        loss_sim = simcse_loss(emb,temp_1=temp_1)\n",
        "        loss_supsim = sup_simcse_loss(emb, labels,temp_2=temp_2)\n",
        "        total_train_loss += loss.item()\n",
        "        loss_value=loss.item()\n",
        "        pred_sig = torch.sigmoid(pred)\n",
        "        preds_detach=torch.round(pred_sig.detach())\n",
        "        accuracy_metric(preds_detach,labels)\n",
        "        loss = loss + lamda*(loss_sim + loss_supsim)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        progress_bar.update(1)\n",
        "    average_epoch_train_loss = total_train_loss / len(train_dataloader)  # Compute average epoch loss\n",
        "    train_accuracy =  accuracy_metric.compute()\n",
        "    accuracy_metric.reset()\n",
        "    return average_epoch_train_loss,train_accuracy.item()"
      ],
      "metadata": {
        "id": "Wi-GKjBYG2nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "w_wARUsKI6td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(best_prams[\"EPOCHS\"]):\n",
        "        start_time = time.time()\n",
        "        average_epoch_train_loss,train_accuracy = training_loop(model, train_dataloader,optimizer,criteon,device,best_prams[\"TEMP_1\"],best_prams[\"TEMP_2\"],best_prams[\"LAMBDA\"])\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
        "        print(f'\\tTrain Loss: {average_epoch_train_loss:.3f} | Train Acc: {train_accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "id": "Wj6NotDMH81X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate with Test Dataset"
      ],
      "metadata": {
        "id": "wsoo9Gm3kB0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(model, test_dataloader,criteon, device,average:str=\"macro\"):\n",
        "    collection_metric = MetricCollection(\n",
        "          BinaryAccuracy(),\n",
        "          MulticlassPrecision(num_classes=2,average=average),\n",
        "          MulticlassRecall(num_classes=2,average=average),\n",
        "          MulticlassF1Score(num_classes=2,average=average),\n",
        "    )\n",
        "    collection_metric.to(device)\n",
        "    bcm_metric = BinaryConfusionMatrix()\n",
        "    bcm_metric.to(device)\n",
        "    model.eval()\n",
        "    total_test_loss = 0.0\n",
        "    for batch in test_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        labels = batch[\"labels\"]\n",
        "        with torch.no_grad():\n",
        "            emb, pred = model(batch , False)\n",
        "            loss = criteon(pred, labels.float())\n",
        "            pred = torch.round(torch.sigmoid(pred))\n",
        "        total_test_loss += loss.item()\n",
        "        collection_metric(pred,labels)\n",
        "        bcm_metric(pred,labels)\n",
        "    average_epoch_test_loss = total_test_loss / len(test_dataloader)  # Compute average epoch loss\n",
        "    result =  collection_metric.compute()\n",
        "    bcm_metric.compute()\n",
        "    collection_metric.reset()\n",
        "    result['Loss']=average_epoch_test_loss\n",
        "    result['confustion_matrix'],_=bcm_metric.plot()\n",
        "    bcm_metric.reset()\n",
        "    return result"
      ],
      "metadata": {
        "id": "SaHndSmFkCsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model with Test Set"
      ],
      "metadata": {
        "id": "K63UySiI3Wfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_metrics=test_loop(model, test_dataloader,criteon, device,average=\"weighted\")"
      ],
      "metadata": {
        "id": "ga8UQq1_kLQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NigPkLTlXSkt",
        "outputId": "8e3abff5-4eb1-4873-93ca-adf2b22266be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BinaryAccuracy': tensor(0.6497, device='cuda:0'),\n",
              " 'MulticlassPrecision': tensor([0.5945, 0.6548], device='cuda:0'),\n",
              " 'MulticlassRecall': tensor([0.1374, 0.9458], device='cuda:0'),\n",
              " 'MulticlassF1Score': tensor([0.2232, 0.7738], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the Test Results to Neptuna"
      ],
      "metadata": {
        "id": "ZpiSM0GW3fw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_test_metrics(result,run: neptune.Run):\n",
        "    run[\"Test_Result/Accuracy\"]=result[\"BinaryAccuracy\"]\n",
        "    run[\"Test_Result/Loss\"]=result[\"Loss\"]\n",
        "    run[\"Test_Result/weighted_Precision\"]=result[\"wMulticlassPrecision\"]\n",
        "    run[\"Test_Result/weighted_Recall\"]=result[\"MulticlassRecall\"]\n",
        "    run[\"Test_Result/weighted_F1Score\"]=result[\"MulticlassF1Score\"]\n",
        "    run[\"Test_Result/confustion_matrix\"].upload(result[\"confustion_matrix\"])\n",
        "    run.wait()\n",
        "    print(\"Upload Succesfull\")\n"
      ],
      "metadata": {
        "id": "R3dpnsRaTJ10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_test_metrics(result,run_study)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIPZ_CE83eIk",
        "outputId": "3089a1b9-46d8-4c67-bbf8-4edea1fb52d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload Succesfull\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_trial.stop()\n",
        "run_study.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I7YstLNUceh",
        "outputId": "233b0cc4-fed9-4e7d-96ea-0388441e6c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "All 0 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/krishanchavinda.official/Fine-Tuning-DCL-Framework/e/FIN-27/metadata\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "All 0 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/krishanchavinda.official/Fine-Tuning-DCL-Framework/e/FIN-26/metadata\n"
          ]
        }
      ]
    }
  ]
}