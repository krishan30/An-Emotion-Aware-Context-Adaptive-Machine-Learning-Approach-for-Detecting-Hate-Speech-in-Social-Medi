{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg8LEnMpMqAg",
        "outputId": "6139274e-48ef-43b3-d090-1e3a6da8431a"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "PlXMphejLaKY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import transformers as tfs\n",
        "from transformers import AutoModel, BertTokenizerFast, RobertaTokenizer, RobertaModel\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 1234\n",
        "batch_size = 256\n",
        "epochs = 1\n",
        "\n",
        "DROPOUT = 0.5\n",
        "ALPHA = 0.3\n",
        "GAMMA = 2\n",
        "TEMP_1 = 0.12\n",
        "TEMP_2 = 0.05\n",
        "\n",
        "#train_path = \"./drive/MyDrive/DCL/preprocess_train.csv\"\n",
        "#dev_path = \"./drive/MyDrive/DCL/preprocess_dev.csv\"\n",
        "#test_path = \"./drive/MyDrive/DCL/preprocess_test.csv\"\n",
        "\n",
        "# train_path = \"./SemEval_Task5/relabel_train.csv\"\n",
        "# dev_path = \"./SemEval_Task5/relabel_dev.csv\"\n",
        "# test_path = \"./SemEval_Task5/relabel_test.csv\"\n",
        "\n",
        "#train_path = \"./drive/MyDrive/hateval/hateval2019_en_train_preprocessed.csv\"\n",
        "#dev_path = \"./drive/MyDrive/hateval/hateval2019_en_dev_preprocessed.csv\"\n",
        "#test_path = \"./drive/MyDrive/hateval/hateval2019_en_test_preprocessed.csv\"\n",
        "\n",
        "#train_path = \"./drive/MyDrive/davidson_dataset/davidson_preprocessed_train.csv\"\n",
        "#dev_path = \"./drive/MyDrive/davidson_dataset/davidson_preprocessed_dev.csv\"\n",
        "#test_path = \"./drive/MyDrive/davidson_dataset/davidson_preprocessed_test.csv\"\n",
        "train_path = \"./drive/MyDrive/HatEval/preprocess_train.csv\"\n",
        "dev_path = \"./drive/MyDrive/HatEval/preprocess_dev.csv\"\n",
        "test_path = \"./drive/MyDrive/HatEval/preprocess_test.csv\""
      ],
      "metadata": {
        "id": "4J80fGrQLs-M"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "setup_seed(SEED)\n",
        "\n",
        "def sen_lable(sen_score):\n",
        "    if sen_score < 0:\n",
        "        return -1\n",
        "    elif sen_score > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# 小写字母\n",
        "def lower_text(text):\n",
        "    if type(text) != str:\n",
        "        return text\n",
        "    return (text.lower())\n",
        "\n",
        "# 数据集\n",
        "def build_dataset(path, batch_size=64, if_train=False, if_test=False):\n",
        "    train_df_ = pd.read_csv(path)\n",
        "    train_df_['label'] = train_df_['label'].astype('int')\n",
        "    # train_df_['HS'] = train_df_['HS'].astype('int')\n",
        "    train_df_ = train_df_.dropna()\n",
        "    if not if_test:\n",
        "        train_df_ = shuffle(train_df_)\n",
        "    text = train_df_[\"tweet\"].values\n",
        "    label = train_df_[\"label\"].values\n",
        "    #if_poison = train_df_[\"if_poison\"].values\n",
        "\n",
        "    batch_train_inputs, batch_train_targets = [], []\n",
        "    # 如果使用对比损失，batch_size折半，生成batch_size个新样本\n",
        "    if if_train:\n",
        "        batch_size = int(batch_size/2)\n",
        "    batch_count = int(round(len(text) / batch_size))\n",
        "    #batch_count = int(math.ceil(len(text) / batch_size))\n",
        "    for i in range(batch_count):\n",
        "        if (len(text) > (i+1)*batch_size):\n",
        "            batch_train_inputs.append(text[i*batch_size : (i+1)*batch_size])\n",
        "            batch_train_targets.append(label[i*batch_size : (i+1)*batch_size])\n",
        "        else:\n",
        "            batch_train_inputs.append(text[i*batch_size:])\n",
        "            batch_train_targets.append(label[i*batch_size:])\n",
        "\n",
        "    return batch_count, [batch_train_inputs, batch_train_targets]\n",
        "\n",
        "def copy_label(labels):\n",
        "    labels = labels.unsqueeze(1)\n",
        "    labels = torch.cat((labels, labels), dim=1).reshape(-1, 1).squeeze(1)\n",
        "    return labels\n",
        "\n"
      ],
      "metadata": {
        "id": "8JCzIrIhLxwS"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch, train_data = build_dataset(train_path, batch_size, if_train=True)\n",
        "val_batch, val_data = build_dataset(dev_path, batch_size)\n",
        "test_batch, test_data = build_dataset(test_path, batch_size, if_test=True)\n",
        "\n",
        "# import BERT-base pretrained model and tokenizer\n",
        "#bert = AutoModel.from_pretrained('bert-base-cased')\n",
        "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "# import roberta\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# bert = RobertaModel.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "IfyBdnfPRY9D"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        model_class, tokenizer_class, pretrained_weights = (tfs.BertModel, tfs.BertTokenizer, 'bert-base-cased')\n",
        "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "        self.bert = model_class.from_pretrained(pretrained_weights)\n",
        "        self.dim = 768\n",
        "        self.dense = nn.Linear(self.dim, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        #self.dropout_2 = nn.Dropout(0.7)\n",
        "\n",
        "    def forward(self, batch_sentences, if_train=False):\n",
        "        batch_tokenized = self.tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=True,\n",
        "                                max_length=45, padding='max_length', truncation=True)      #tokenize、add special token、pad\n",
        "        input_ids = torch.tensor(batch_tokenized['input_ids']).to(device)\n",
        "        attention_mask = torch.tensor(batch_tokenized['attention_mask']).to(device)\n",
        "\n",
        "        bert_output = self.bert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        #bert_cls_hidden_state = bert_output[0][:,0,:]   #提取[CLS]对应的隐藏状态\n",
        "        bert_cls_hidden_state = bert_output[1]  #提取pooled后的句向量\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # hidden_states = bert_output[2][-1]\n",
        "        # bert_cls_hidden_state = torch.mean(hidden_states, 1)\n",
        "\n",
        "        # 如果使用无监督对比学习，利用dropout生成句向量的正样本，样本总量*2\n",
        "        if if_train:\n",
        "            bert_cls_hidden_state_copy = self.dropout(bert_cls_hidden_state)\n",
        "            bert_cls_hidden_state = torch.cat((bert_cls_hidden_state, bert_cls_hidden_state_copy), dim=1).reshape(-1, self.dim)\n",
        "        else:\n",
        "            bert_cls_hidden_state = self.dropout(bert_cls_hidden_state)\n",
        "\n",
        "        linear_output = self.dense(bert_cls_hidden_state)\n",
        "        linear_output = linear_output.squeeze(1)\n",
        "\n",
        "        return bert_cls_hidden_state, linear_output"
      ],
      "metadata": {
        "id": "V8REmlEYRf3n"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.4, gamma=2, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = torch.tensor(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "\n",
        "        device = target.device\n",
        "        self.alpha = self.alpha.to(device)\n",
        "\n",
        "        pred = nn.Sigmoid()(pred)\n",
        "        pred = pred.view(-1, 1)\n",
        "        target = target.view(-1, 1)\n",
        "        pred = torch.cat((1-pred, pred), dim=1)\n",
        "\n",
        "        class_mask = torch.zeros(pred.shape[0], pred.shape[1]).to(device)\n",
        "        class_mask.scatter_(1, target.view(-1, 1).long(), 1.)\n",
        "        probs = (pred * class_mask).sum(dim=1).view(-1, 1)\n",
        "        probs = probs.clamp(min=0.0001, max=1.0)\n",
        "\n",
        "        log_p = probs.log()\n",
        "        alpha = torch.ones(pred.shape[0], pred.shape[1]).to(device)\n",
        "        alpha[:, 0] = alpha[:, 0] * (1 - self.alpha)\n",
        "        alpha[:, 1] = alpha[:, 1] * self.alpha\n",
        "        alpha = (alpha * class_mask).sum(dim=1).view(-1, 1)\n",
        "\n",
        "        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p\n",
        "\n",
        "        if self.size_average:\n",
        "            loss = batch_loss.mean()\n",
        "        else:\n",
        "            loss = batch_loss.sum()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "lZW93tm7RnwM"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsupervised Contrastive loss\n",
        "def simcse_loss(batch_emb):\n",
        "    # 构造标签\n",
        "    batch_size = batch_emb.size(0)\n",
        "    y_true = torch.cat([torch.arange(1, batch_size, step=2, dtype=torch.long).unsqueeze(1),\n",
        "                        torch.arange(0, batch_size, step=2, dtype=torch.long).unsqueeze(1)],\n",
        "                       dim=1).reshape([batch_size,]).to(device)\n",
        "\n",
        "    # 计算score和loss\n",
        "    norm_emb = F.normalize(batch_emb, dim=1, p=2)\n",
        "    sim_score = torch.matmul(norm_emb, norm_emb.transpose(0,1))  # 句向量点积\n",
        "    sim_score = sim_score - (torch.eye(batch_size) * 1e12).to(device)\n",
        "    sim_score = sim_score / TEMP_1  # 温度系数为 0.05，也就是乘以20\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    loss = loss_func(sim_score, y_true)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Supervised Contrastive loss\n",
        "def sup_simcse_loss(batch_emb, label):\n",
        "    n = batch_emb.size(0)\n",
        "\n",
        "    similarity_matrix = F.cosine_similarity(batch_emb.unsqueeze(1), batch_emb.unsqueeze(0), dim=2)\n",
        "    mask = torch.ones_like(similarity_matrix) * (label.expand(n, n).eq(label.expand(n, n).t()))\n",
        "\n",
        "    mask_no_sim = torch.ones_like(mask) - mask\n",
        "    mask_dui_jiao_0 = ((torch.ones(n,n) - torch.eye(n,n)) * 1e12).to(device)\n",
        "    similarity_matrix = torch.exp(similarity_matrix/TEMP_2)\n",
        "    #print(similarity_matrix)\n",
        "    similarity_matrix = similarity_matrix * mask_dui_jiao_0\n",
        "\n",
        "    sim = mask*similarity_matrix\n",
        "    no_sim = similarity_matrix - sim\n",
        "    no_sim_sum = torch.sum(no_sim , dim=1)\n",
        "    no_sim_sum_expend = no_sim_sum.repeat(n, 1).T\n",
        "\n",
        "    sim_sum  = sim + no_sim_sum_expend\n",
        "    loss = torch.div(sim , sim_sum)\n",
        "    loss = mask_no_sim + loss + (torch.eye(n, n)/1e12).to(device)\n",
        "    loss = -torch.log(loss)\n",
        "    loss = torch.sum(torch.sum(loss, dim=1))/(2*n)\n",
        "    #loss = loss/100\n",
        "    return loss\n",
        "\n",
        "model = BERT_Arch().to(device)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr = 1e-4)\n",
        "#optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
        "\n",
        "#criteon = nn.NLLLoss(weight=weights)\n",
        "#criteon = nn.BCEWithLogitsLoss()\n",
        "##criteon = nn.NLLLoss()\n",
        "criteon = FocalLoss(ALPHA, GAMMA)"
      ],
      "metadata": {
        "id": "juwhPHIGR0F3"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_acc(preds, y):\n",
        "    preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = torch.eq(preds, y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return preds, acc\n",
        "\n",
        "# def pred_lable(preds):\n",
        "#     _, preds = torch.max(preds, 1)\n",
        "#     pred_lables = []\n",
        "#     return pred_lables\n",
        "\n",
        "# 训练函数\n",
        "def train(model, batch_count, batch_data, optimizer, criteon):\n",
        "    batch_train_inputs, batch_train_targets = batch_data[0], batch_data[1]\n",
        "    avg_loss = []\n",
        "    avg_acc = []\n",
        "    model.train()\n",
        "    step = 0\n",
        "\n",
        "    for i in tqdm(range(batch_count)):\n",
        "        inputs = batch_train_inputs[i]\n",
        "        labels = torch.tensor(batch_train_targets[i]).to(device)\n",
        "        labels = copy_label(labels)\n",
        "        # print(inputs[:20])\n",
        "        # break\n",
        "        emb, pred = model(inputs, if_train=True)\n",
        "        #emb, pred = model(inputs)\n",
        "\n",
        "        loss = criteon(pred, labels.float())\n",
        "        loss_sim = simcse_loss(emb)\n",
        "        loss_supsim = sup_simcse_loss(emb, labels)\n",
        "\n",
        "        _, acc = binary_acc(pred, labels)\n",
        "        avg_loss.append(loss.item())\n",
        "        avg_acc.append(acc.item())\n",
        "\n",
        "        loss = loss + (loss_sim + loss_supsim)\n",
        "        #loss = loss + loss_supsim\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        step += 1\n",
        "\n",
        "    avg_loss = np.array(avg_loss).mean()\n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "# 评估函数\n",
        "def eval(model, batch_count, batch_data, criteon):\n",
        "    batch_train_inputs, batch_train_targets = batch_data[0], batch_data[1]\n",
        "    avg_loss = []\n",
        "    avg_acc = []\n",
        "    total_labels = []\n",
        "    total_preds = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(batch_count)):\n",
        "            inputs = batch_train_inputs[i]\n",
        "            labels = torch.tensor(batch_train_targets[i]).to(device)\n",
        "            emb, pred = model(inputs, False)\n",
        "\n",
        "            loss = criteon(pred, labels.float())\n",
        "            preds, acc = binary_acc(pred, labels)\n",
        "            avg_loss.append(loss.item())\n",
        "            avg_acc.append(acc.item())\n",
        "\n",
        "            pred_lables = preds.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            total_preds.extend(pred_lables)\n",
        "            total_labels.extend(labels)\n",
        "\n",
        "    print(total_labels[100:120])\n",
        "    print(total_preds[100:120])\n",
        "    avg_loss = np.array(avg_loss).mean()\n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    print(classification_report(total_labels, total_preds, digits = 4))\n",
        "    return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "L0InWzI2L5d_"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_acc = float('-inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_batch, train_data, optimizer, criteon)\n",
        "    dev_loss, dev_acc = eval(model, val_batch, val_data, criteon)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "\n",
        "    if dev_acc > best_valid_acc:          #只要模型效果变好，就保存\n",
        "        best_valid_acc = dev_acc\n",
        "        torch.save(model.state_dict(), 'wordavg-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val Loss: {dev_loss:.3f} |  Val Acc: {dev_acc*100:.2f}%')\n",
        "\n",
        "#用保存的模型参数预测数据\n",
        "model.load_state_dict(torch.load(\"wordavg-model.pt\"))\n",
        "test_loss, test_acc = eval(model, test_batch, test_data, criteon)\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')\n",
        "\n",
        "# from transformers_interpret import SequenceClassificationExplainer\n",
        "# cls_explainer = SequenceClassificationExplainer(\n",
        "#     model,\n",
        "#     tokenizer)\n",
        "# word_attributions = cls_explainer(\"I love you, I like you\")\n",
        "# print(word_attributions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj4M2m1DSDGV",
        "outputId": "714de51c-e37b-4c9b-a109-9a3a79437ec0"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [01:13<00:00,  1.05s/it]\n",
            "100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6971    0.9213    0.7937       572\n",
            "           1     0.8148    0.4637    0.5910       427\n",
            "\n",
            "    accuracy                         0.7257       999\n",
            "   macro avg     0.7560    0.6925    0.6924       999\n",
            "weighted avg     0.7474    0.7257    0.7071       999\n",
            "\n",
            "Epoch: 01 | Epoch Time: 1.0m 16.72s\n",
            "\tTrain Loss: 0.081 | Train Acc: 60.02%\n",
            "\t Val Loss: 0.059 |  Val Acc: 72.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:08<00:00,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
            "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7509    0.6751    0.7110      1625\n",
            "           1     0.6071    0.6915    0.6466      1180\n",
            "\n",
            "    accuracy                         0.6820      2805\n",
            "   macro avg     0.6790    0.6833    0.6788      2805\n",
            "weighted avg     0.6904    0.6820    0.6839      2805\n",
            "\n",
            "Test Loss: 0.077 |  Test Acc: 68.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}